{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Log for setup or undo of database pgnetworks\n",
      "\n",
      "\n",
      "INFO: secret.txt already exists.\n",
      "INFO: setup_statements.sql has been created.\n",
      "INFO: undo_statements.sql has been created.\n",
      "INFO: Database pgnetworks already exists.\n",
      "INFO: Extension fuzzystrmatch exists.\n",
      "INFO: Extension pg_trgm exists.\n",
      "INFO: Extension postgis exists.\n",
      "INFO: Extension pgrouting exists.\n",
      "INFO: Extension plpython3u exists.\n",
      "INFO: Extension pgcrypto exists.\n",
      "INFO: Extension btree_gin exists.\n",
      "INFO: Extension h3 has been installed.\n",
      "INFO: User administrator already exists in the database.\n",
      "INFO: User routing already exists in the database.\n",
      "INFO: User daues_m already exists in the database.\n",
      "['pg_database_owner', 'pg_read_all_data', 'pg_write_all_data', 'pg_monitor', 'pg_read_all_settings', 'pg_read_all_stats', 'pg_stat_scan_tables', 'pg_read_server_files', 'pg_write_server_files', 'pg_execute_server_program', 'pg_signal_backend', 'pg_checkpoint', 'postgres', 'gis', 'new_schema_use', 'new_schema_user', 'testuser', 'airflow', 'administrator', 'routing', 'daues_m', 'pgnetworks_staging_all', 'pgnetworks_staging_use', 'pgnetworks_staging_r', 'pgnetworks_all', 'pgnetworks_use', 'pgnetworks_r', 'public_all', 'public_use', 'public_r']\n",
      "INFO: Schema pgnetworks_staging exists.\n",
      "INFO: Role pgnetworks_staging_all already exists on the cluster.\n",
      "INFO: grant pgnetworks_staging_all to postgres; committed\n",
      "INFO: grant all on schema pgnetworks_staging to pgnetworks_staging_all; committed\n",
      "INFO: grant all on all functions in schema pgnetworks_staging to pgnetworks_staging_all; committed\n",
      "INFO: Role pgnetworks_staging_use already exists on the cluster.\n",
      "INFO: grant pgnetworks_staging_use to postgres; committed\n",
      "INFO: grant usage on schema pgnetworks_staging to pgnetworks_staging_use; committed\n",
      "INFO: Role pgnetworks_staging_r already exists on the cluster.\n",
      "INFO: grant pgnetworks_staging_r to postgres; committed\n",
      "INFO: grant usage on schema pgnetworks_staging to pgnetworks_staging_r; committed\n",
      "INFO: Schema pgnetworks exists.\n",
      "INFO: Role pgnetworks_all already exists on the cluster.\n",
      "INFO: grant pgnetworks_all to postgres; committed\n",
      "INFO: grant all on schema pgnetworks to pgnetworks_all; committed\n",
      "INFO: grant all on all functions in schema pgnetworks to pgnetworks_all; committed\n",
      "INFO: Role pgnetworks_use already exists on the cluster.\n",
      "INFO: grant pgnetworks_use to postgres; committed\n",
      "INFO: grant usage on schema pgnetworks to pgnetworks_use; committed\n",
      "INFO: Role pgnetworks_r already exists on the cluster.\n",
      "INFO: grant pgnetworks_r to postgres; committed\n",
      "INFO: grant usage on schema pgnetworks to pgnetworks_r; committed\n",
      "INFO: Schema public exists.\n",
      "INFO: Role public_all already exists on the cluster.\n",
      "INFO: grant public_all to postgres; committed\n",
      "INFO: grant all on schema public to public_all; committed\n",
      "INFO: grant all on all functions in schema public to public_all; committed\n",
      "INFO: Role public_use already exists on the cluster.\n",
      "INFO: grant public_use to postgres; committed\n",
      "INFO: grant usage on schema public to public_use; committed\n",
      "INFO: Role public_r already exists on the cluster.\n",
      "INFO: grant public_r to postgres; committed\n",
      "INFO: grant usage on schema public to public_r; committed\n",
      "INFO: User administrator has been granted privilege pgnetworks_all\n",
      "INFO: Default privileges for future objects in schema altered.\n",
      "INFO: User administrator has been granted privilege pgnetworks_staging_all\n",
      "INFO: Default privileges for future objects in schema altered.\n",
      "INFO: User administrator has been granted privilege public_all\n",
      "INFO: Default privileges for future objects in schema altered.\n",
      "ERROR: (psycopg2.errors.UndefinedObject) role \"pgnetworks_read\" does not exist\n",
      "\n",
      "[SQL: grant pgnetworks_read to routing;]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "ERROR: (psycopg2.errors.UndefinedObject) role \"public_read\" does not exist\n",
      "\n",
      "[SQL: grant public_read to routing;]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "INFO: User daues_m has been granted privilege pgnetworks_all\n",
      "INFO: Default privileges for future objects in schema altered.\n",
      "INFO: User daues_m has been granted privilege pgnetworks_staging_all\n",
      "INFO: Default privileges for future objects in schema altered.\n",
      "INFO: User daues_m has been granted privilege public_all\n",
      "INFO: Default privileges for future objects in schema altered.\n"
     ]
    }
   ],
   "source": [
    "import simple_postgres_setup as sps\n",
    "\n",
    "dir(sps)\n",
    "\n",
    "#sps.drop_database('inputs/config.yml')\n",
    "sps.setup_database('inputs/config.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load libraries and define db_connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "import aiosql\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# import environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# import sql from folder\n",
    "queries = aiosql.from_path(\"./sql\", psycopg2)\n",
    "\n",
    "# prepare db connection\n",
    "user = os.getenv('USER')\n",
    "pwd  = os.getenv('PASS')\n",
    "host = os.getenv('HOST')\n",
    "port = os.getenv('PORT')\n",
    "db   = os.getenv('DB')\n",
    "connect_db = f\"postgresql://{user}:{pwd}@{host}:{port}/{db}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tear down and set stuff up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg2.connect(connect_db) as conn:\n",
    "    try:\n",
    "        # drop assets\n",
    "        queries.ddl.drop_table_vertex_2_edge(conn)\n",
    "        queries.ddl.drop_table_junctioned_edges(conn)\n",
    "        queries.ddl.drop_type_segments_processing(conn)\n",
    "        queries.ddl.drop_table_segments(conn)\n",
    "        queries.ddl.drop_type_edge_processing(conn)\n",
    "        #queries.ddl.drop_table_log(conn)\n",
    "        \n",
    "        # rebuild assets\n",
    "        queries.ddl.create_table_vertex_2_edge(conn)\n",
    "        queries.ddl.create_table_junctioned_edges(conn)\n",
    "        queries.ddl.create_table_segments(conn)\n",
    "        queries.ddl.create_type_segments_processing(conn)\n",
    "        queries.ddl.create_type_edge_processing(conn)\n",
    "        #queries.ddl.create_table_log(conn)\n",
    "        \n",
    "        # create and replace assets\n",
    "        queries.dml.create_procedure_join_vertex_2_edge(conn)\n",
    "        queries.dml.create_procedure_process_junctions_and_edges(conn)\n",
    "        \n",
    "        conn.commit()\n",
    "    \n",
    "    except psycopg2.Error as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download sources and copy data to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess data in the DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *define global variables for the run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the processing variables\n",
    "\n",
    "# run start\n",
    "# execute workstep \n",
    "run_start_date = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "# chunk_size  = the batch size for each partial process\n",
    "chunk_size = 100000\n",
    "\n",
    "# reduce chunk_size for the enhancement and segmentation process\n",
    "edge_processing_chunk_size = int(chunk_size / 1)\n",
    "\n",
    "# concurrency = the number of parallel processes \n",
    "concurrency = 6\n",
    "\n",
    "# run_id\n",
    "run_id = int(time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *define the wrapper functions for multiprocessing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### *joining the vertices to the closest input edge*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_join_vertex_2_edge(lower_bound: int, upper_bound: int, chunk_size: int, run_id: int):\n",
    "    \"\"\"\n",
    "    Call the junctioning procedure that \n",
    "    joins every POI to its closest edge\n",
    "    \"\"\"\n",
    "    params = (lower_bound, upper_bound, chunk_size, run_id)\n",
    "    with psycopg2.connect(connect_db) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(queries.dml.join_vertex_2_edge.sql, params)\n",
    "        conn.commit()\n",
    "\n",
    "def parallel_call_join_vertex_2_edge(params_list, concurrency):\n",
    "    \"\"\"\n",
    "    Execute the procedure for each\n",
    "    chunk in parallel\n",
    "    \"\"\"\n",
    "    with mp.Pool(processes=concurrency) as pool:\n",
    "        pool.starmap(call_join_vertex_2_edge,params_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### *enhancing and segmentizing the input edges*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_process_junctions_and_edges(lower_bound: int, upper_bound: int, chunk_size: int, run_id: int):\n",
    "    \"\"\"\n",
    "    Call the junctioning procedure that \n",
    "    joins every POI to its closest edge\n",
    "    \"\"\"\n",
    "    params = (lower_bound, upper_bound, chunk_size, run_id)\n",
    "    with psycopg2.connect(connect_db) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(queries.dml.process_junctions_and_edges.sql, params)\n",
    "        conn.commit()\n",
    "\n",
    "def parallel_call_process_junctions_and_edges(params_list, concurrency):\n",
    "    \"\"\"\n",
    "    Execute the procedure for each\n",
    "    chunk in parallel\n",
    "    \"\"\"\n",
    "    with mp.Pool(processes=concurrency) as pool:\n",
    "        pool.starmap(call_process_junctions_and_edges,params_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *execute processing steps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### *parallel step: joining vertices to closest edge*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the list of lower bound IDs to\n",
    "# select the chunks for parallel processing\n",
    "# for the workstep \"join_vertices_2_edge\"\n",
    "\n",
    "work_step = 'identify_chunks_for_join_vertices_2_edge'\n",
    "start_date = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "with psycopg2.connect(connect_db) as conn:\n",
    "    bounds = list(queries.dml.find_bounds_in_poi_table(conn, chunk_size=chunk_size))\n",
    "\n",
    "bounds_list = []\n",
    "for row in bounds:\n",
    "    bound = row[0]\n",
    "    bounds_list.append(bound)\n",
    "\n",
    "params_list = [(bounds_list[i], bounds_list[i+1], chunk_size, run_id) for i in range(len(bounds_list)-1)]\n",
    "i = len(bounds_list)-1\n",
    "params_list.append((bounds_list[i],bounds_list[i]+1, chunk_size, run_id))\n",
    "params_list=params_list[:36]\n",
    "#params_list\n",
    "\n",
    "end_date = datetime.now(timezone.utc).isoformat()\n",
    "message = '{\"idx\":1}'\n",
    "log_level = \"INFO\"\n",
    "with psycopg2.connect(connect_db) as conn:\n",
    "    queries.dml.write_to_log(conn,log_level=log_level,run_id=run_id,start_date=start_date,end_date=end_date,work_step=work_step,chunk_size=chunk_size,message=message)\n",
    "    conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute workstep \n",
    "work_step = 'join_vertex_2_edge'\n",
    "start_date = datetime.now(timezone.utc).isoformat()\n",
    "parallel_call_join_vertex_2_edge(params_list,concurrency)\n",
    "end_date = datetime.now(timezone.utc).isoformat()\n",
    "message = '{\"idx\":2}'\n",
    "log_level = \"INFO\"\n",
    "with psycopg2.connect(connect_db) as conn:\n",
    "    queries.dml.write_to_log(conn,log_level=log_level,run_id=run_id,start_date=start_date,end_date=end_date,work_step=work_step,chunk_size=chunk_size,message=message)\n",
    "    conn.commit()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### *serial step: index vertex_2_edge on edge_id for further processing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index on vertex_2_edge (id)\n",
    "work_step = 'create_index_on_vertex_2_edge'\n",
    "start_date = datetime.now(timezone.utc).isoformat()\n",
    "with psycopg2.connect(connect_db) as conn:\n",
    "    queries.ddl.create_index_vertex_2_edge_edge_id_idx(conn)\n",
    "    conn.commit()   \n",
    "end_date = datetime.now(timezone.utc).isoformat()\n",
    "message = '{\"idx\":3}'\n",
    "log_level = \"INFO\"\n",
    "with psycopg2.connect(connect_db) as conn:\n",
    "    queries.dml.write_to_log(conn,log_level=log_level,run_id=run_id,start_date=start_date,end_date=end_date,work_step=work_step,chunk_size=chunk_size,message=message)\n",
    "    conn.commit()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### *parallel step: enhance and segmentize the input edges* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the list of lower bound IDs to\n",
    "# select the chunks for parallel processing\n",
    "# for the work_step \"process_junctions_end_edges\"\n",
    "\n",
    "work_step = 'identify_chunks_for_process_junctions_end_edges'\n",
    "start_date = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "\n",
    "with psycopg2.connect(connect_db) as conn:\n",
    "    bounds = list(queries.dml.find_bounds_in_vertex_2_edge(conn, chunk_size=edge_processing_chunk_size))\n",
    "\n",
    "bounds_list = []\n",
    "for row in bounds:\n",
    "    bound = row[0]\n",
    "    bounds_list.append(bound)\n",
    "\n",
    "params_list = [(bounds_list[i], bounds_list[i+1], edge_processing_chunk_size, run_id) for i in range(len(bounds_list)-1)]\n",
    "i = len(bounds_list)-1\n",
    "params_list.append((bounds_list[i],bounds_list[i]+1, edge_processing_chunk_size, run_id))\n",
    "#params_list=params_list[:10]\n",
    "#params_list\n",
    "\n",
    "end_date = datetime.now(timezone.utc).isoformat()\n",
    "message = '{\"idx\":4}'\n",
    "log_level = \"INFO\"\n",
    "with psycopg2.connect(connect_db) as conn:\n",
    "    queries.dml.write_to_log(conn,log_level=log_level,run_id=run_id,start_date=start_date,end_date=end_date,work_step=work_step,chunk_size=chunk_size,message=message)\n",
    "    conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute workstep \"process_junctions_and_edges\"\n",
    "work_step = 'process_junctions_and_edges'\n",
    "start_date = datetime.now(timezone.utc).isoformat()\n",
    "parallel_call_process_junctions_and_edges(params_list,concurrency)\n",
    "end_date = datetime.now(timezone.utc).isoformat()\n",
    "message = '{\"idx\":5}'\n",
    "log_level = \"INFO\"\n",
    "with psycopg2.connect(connect_db) as conn:\n",
    "    queries.dml.write_to_log(conn,log_level=log_level,run_id=run_id,start_date=start_date,end_date=end_date,work_step=work_step,chunk_size=chunk_size,message=message)\n",
    "    conn.commit() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### *serial step: index segments on geometry for further processing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create geometry index on segments\n",
    "work_step = 'create_geometry_index_on_segments'\n",
    "start_date = datetime.now(timezone.utc).isoformat()  \n",
    "with psycopg2.connect(connect_db) as conn:\n",
    "    queries.ddl.create_index_segments_geom_idx(conn)\n",
    "    conn.commit() \n",
    "end_date = datetime.now(timezone.utc).isoformat()\n",
    "message = '{\"idx\":6}'\n",
    "log_level = \"INFO\"\n",
    "with psycopg2.connect(connect_db) as conn:\n",
    "    queries.dml.write_to_log(conn,log_level=log_level,run_id=run_id,start_date=start_date,end_date=end_date,work_step=work_step,chunk_size=chunk_size,message=message)\n",
    "    conn.commit()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run log entry\n",
    "\n",
    "# run end\n",
    "work_step = 'total_run'\n",
    "start_date = run_start_date\n",
    "end_date = datetime.now(timezone.utc).isoformat()\n",
    "message = {\"idx\":0,\n",
    "           \"concurrency\": concurrency,\n",
    "           \"chunk_size\": chunk_size,\n",
    "           \"edge_processing_chunk_size\": edge_processing_chunk_size\n",
    "           }\n",
    "message = json.dumps(message)\n",
    "log_level = \"INFO\"\n",
    "with psycopg2.connect(connect_db) as conn:\n",
    "    queries.dml.write_to_log(conn,log_level=log_level,run_id=run_id,start_date=start_date,end_date=end_date,work_step=work_step,chunk_size=chunk_size,message=message)\n",
    "    conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for performance display\n",
    "with psycopg2.connect(connect_db) as conn:\n",
    "    with queries.dml.avg_workstep_duration_item_count_cursor(conn) as cur:\n",
    "        rows = cur.fetchall()\n",
    "        col_names = [desc[0] for desc in cur.description]\n",
    "df=pd.DataFrame(rows, columns=col_names)\n",
    "\n",
    "# Create a scatter plot\n",
    "chart = (\n",
    "    alt.Chart(df)\n",
    "    .mark_point()\n",
    "    .encode(\n",
    "        x=alt.X('avg_item_count').scale(type=\"log\"),\n",
    "        y=alt.Y('avg_duration').scale(type=\"linear\"),\n",
    "        color='concurrency',\n",
    "        shape='work_step',\n",
    "        tooltip=[]\n",
    "    )\n",
    "    .properties(\n",
    "        width=800,\n",
    "        height=300,\n",
    "        title='Scatter Plot of item_count vs. duration'\n",
    "    ).interactive()\n",
    ")\n",
    "\n",
    "jchart = alt.JupyterChart(chart)\n",
    "jchart\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
