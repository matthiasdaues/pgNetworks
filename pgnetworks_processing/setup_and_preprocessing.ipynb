{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import simple_postgres_setup as sps\n",
    "\n",
    "# dir(sps)\n",
    "\n",
    "# sps.drop_database('inputs/config.yml')\n",
    "# sps.setup_database('inputs/config.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m provide_db_connect_and_queries\n\u001b[0;32m----> 3\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mprovide_db_connect_and_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "from python.utilities import provide_db_connection_and_queries\n",
    "\n",
    "provide_db_connection_and_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load libraries and define db_connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "query dml.create_function_public_ghh_decode_id_to_hash at sql/dml/ghh_functions.sql:1 may not be a select, consider adding an operator, eg '!'\n",
      "query dml.drop_function_public_ghh_decode_id_to_hash at sql/dml/ghh_functions.sql:35 may not be a select, consider adding an operator, eg '!'\n",
      "query dml.create_function_public_ghh_decode_hash_to_wkt at sql/dml/ghh_functions.sql:41 may not be a select, consider adding an operator, eg '!'\n",
      "query dml.drop_function_public_ghh_decode_hash_to_wkt at sql/dml/ghh_functions.sql:60 may not be a select, consider adding an operator, eg '!'\n",
      "query dml.create_function_public_ghh_decode_id_to_wkt at sql/dml/ghh_functions.sql:66 may not be a select, consider adding an operator, eg '!'\n",
      "query dml.drop_function_public_ghh_decode_id_to_wkt at sql/dml/ghh_functions.sql:92 may not be a select, consider adding an operator, eg '!'\n",
      "query dml.create_function_public_ghh_encode_xy_to_id at sql/dml/ghh_functions.sql:98 may not be a select, consider adding an operator, eg '!'\n",
      "query dml.drop_function_public_ghh_encode_xy_to_id at sql/dml/ghh_functions.sql:117 may not be a select, consider adding an operator, eg '!'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "import aiosql\n",
    "import multiprocessing as mp\n",
    "from dotenv import load_dotenv\n",
    "from operator import itemgetter\n",
    "\n",
    "# import environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# import sql from folder\n",
    "queries = aiosql.from_path(\"./sql\", psycopg2)\n",
    "\n",
    "# prepare db connection\n",
    "user = os.getenv('PROCESS_USER')\n",
    "pwd  = os.getenv('PROCESS_PWD')\n",
    "host = os.getenv('HOST')\n",
    "port = os.getenv('PORT')\n",
    "db   = os.getenv('PROCESS_DB')\n",
    "connect_db = f\"postgresql://{user}:{pwd}@{host}:{port}/{db}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://administrator:ub3uG61QSss$ho9ryqnu9ybtpVcAbw@datenschoenheit.de:25433/pgnetworks\n"
     ]
    }
   ],
   "source": [
    "print(connect_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tear down and set stuff up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg2.connect(connect_db) as conn:\n",
    "    try:\n",
    "#       # drop assets\n",
    "        # queries.ddl.drop_table_vertex_2_edge(conn)\n",
    "        # queries.ddl.drop_table_junctioned_edges(conn)\n",
    "        # queries.ddl.drop_table_segments(conn)\n",
    "        # queries.ddl.drop_type_segment_processing(conn)\n",
    "        # queries.ddl.drop_table_nodes(conn)\n",
    "        # queries.ddl.drop_type_edge_processing(conn)\n",
    "        # queries.ddl.drop_type_edge_processing_2(conn)\n",
    "        # queries.ddl.drop_table_selector_grid(conn)\n",
    "#       # queries.ddl.drop_table_log(conn)\n",
    "\n",
    "#       # drop assets\n",
    "#       # queries.dml.drop_function_public_ghh_decode_id_to_wkt(conn)\n",
    "#       # queries.dml.drop_function_public_ghh_encode_xy_to_id(conn)\n",
    "        queries.dml.drop_procedure_join_vertex_2_edge(conn)\n",
    "        queries.dml.drop_procedure_process_junctions_and_edges(conn)\n",
    "        queries.dml.drop_procedure_calculate_selector_grid(conn)\n",
    "        queries.dml.drop_procedure_segmentize_road_network(conn)\n",
    "        queries.dml.drop_procedure_count_node_degree(conn)\n",
    "        \n",
    "#       # rebuild assets\n",
    "        # queries.ddl.create_table_vertex_2_edge(conn)\n",
    "        # queries.ddl.create_table_junctioned_edges(conn)\n",
    "        # queries.ddl.create_type_segment_processing(conn)\n",
    "        # queries.ddl.create_table_segments(conn)\n",
    "        # queries.ddl.create_table_nodes(conn)\n",
    "        # queries.ddl.create_type_edge_processing(conn)\n",
    "        # queries.ddl.create_type_edge_processing_2(conn)\n",
    "        # queries.ddl.create_table_selector_grid(conn)\n",
    "#       # queries.ddl.create_table_log(conn)\n",
    "        \n",
    "#       # create and replace assets\n",
    "        queries.dml.create_procedure_join_vertex_2_edge(conn)\n",
    "        queries.dml.create_procedure_process_junctions_and_edges(conn)\n",
    "        queries.dml.create_procedure_calculate_selector_grid(conn)\n",
    "        queries.dml.create_procedure_segmentize_road_network(conn)\n",
    "        queries.dml.create_procedure_count_node_degree(conn)\n",
    "        \n",
    "        conn.commit()\n",
    "    \n",
    "    except psycopg2.Error as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download sources and copy data to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess data in the DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *define global variables for the run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the processing variables\n",
    "\n",
    "# run start\n",
    "# execute workstep \n",
    "RUN_START_DATE = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "# chunk_size  = the batch size for each partial process\n",
    "CHUNK_SIZE = 100000\n",
    "\n",
    "# set chunk_size for the enhancement and segmentation process\n",
    "EDGE_PROCESSING_CHUNK_SIZE = 10000 # int(chunk_size / 10)\n",
    "\n",
    "# set chunk_size for the segmentization of the remaining road_network\n",
    "FAR_NET_PROCESSING_CHUNK_SIZE = 20000 # int(chunk_size / 10)\n",
    "\n",
    "# concurrency = the number of parallel processes \n",
    "CONCURRENCY = 12\n",
    "\n",
    "# run_id\n",
    "RUN_ID = int(time.time())\n",
    "\n",
    "# initiate the workstep_idx\n",
    "workstep_idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *define the wrapper functions for multiprocessing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parameter list for a parallel processing work step based on chunk size setting\n",
    "\n",
    "def create_workstep_params_list(chunk_bound_query_name: str, chunk_size: int, workstep_query_name: str, workstep_idx: int, RUN_ID: int):\n",
    "    \"\"\"\n",
    "    create the params_list for the next process\n",
    "    work step for parallel execution.\n",
    "    \"\"\"\n",
    "    # get start_date\n",
    "    start_date = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    # get chunk bounds based on chunk_size\n",
    "    with psycopg2.connect(connect_db) as conn:\n",
    "        chunk_bound_query = getattr(queries.dml, chunk_bound_query_name)\n",
    "        bounds_list = list(map(itemgetter(0),chunk_bound_query(conn, chunk_size=chunk_size)))\n",
    "\n",
    "    # concatenate the params_list\n",
    "    params_list = [(workstep_query_name, bounds_list[i], bounds_list[i+1], chunk_size, RUN_ID) for i in range(len(bounds_list)-1)]\n",
    "    i = len(bounds_list)-1\n",
    "    params_list.append((workstep_query_name, bounds_list[i],bounds_list[i]+1, chunk_size, RUN_ID))\n",
    "\n",
    "    # get end_date\n",
    "    end_date = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    # collect the log info\n",
    "    message = {\"idx\":workstep_idx}\n",
    "    message = json.dumps(message)\n",
    "    log_level = \"INFO\"\n",
    "\n",
    "    # write to log\n",
    "    with psycopg2.connect(connect_db) as conn:\n",
    "        queries.dml.write_to_log(conn,log_level=log_level,run_id=RUN_ID,start_date=start_date,end_date=end_date,work_step=chunk_bound_query_name,chunk_size=chunk_size,item_count=None,message=message)\n",
    "        conn.commit()\n",
    "\n",
    "    return params_list\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid of cells that each contain a maximum of elements for further processing\n",
    "\n",
    "def calculate_selector_grid(max_elements: int):\n",
    "    \"\"\" \n",
    "    \n",
    "    \"\"\"\n",
    "    # get start_date\n",
    "    start_date = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    # retrieve all selector grids\n",
    "    params = (max_elements,)\n",
    "    print(params)\n",
    "    create_grid_statement_name = 'calculate_selector_grid'\n",
    "    create_grid_statement = getattr(queries.dml, create_grid_statement_name).sql\n",
    "    with psycopg2.connect(connect_db) as conn:\n",
    "        start_date = datetime.now(timezone.utc).isoformat()\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(create_grid_statement,params)\n",
    "        conn.commit()   \n",
    "        end_date = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    # collect the log info\n",
    "    message = {\"idx\":workstep_idx}\n",
    "    message = json.dumps(message)\n",
    "    log_level = \"INFO\"\n",
    "\n",
    "    # write to log\n",
    "    with psycopg2.connect(connect_db) as conn:\n",
    "        queries.dml.write_to_log(conn,log_level=log_level,run_id=RUN_ID,start_date=start_date,end_date=end_date,work_step='create_selector_grid',chunk_size=max_elements,item_count=None,message=message)\n",
    "        conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parameter list for a parallel processing work step based on a spatial selector grid\n",
    "\n",
    "def create_spatial_workstep_params_list(spatial_bound_query_name: str, chunk_size: int, workstep_query_name: str, workstep_idx: int, RUN_ID: int):\n",
    "    \"\"\"\n",
    "    create the params_list for the next process\n",
    "    work step for parallel execution.\n",
    "    \"\"\"\n",
    "    # get start_date\n",
    "    start_date = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    # retrieve all selector grids\n",
    "    with psycopg2.connect(connect_db) as conn:\n",
    "        spatial_bound_query = getattr(queries.dml, spatial_bound_query_name)\n",
    "        bounds_list = list(map(itemgetter(0),spatial_bound_query(conn, chunk_size=chunk_size)))\n",
    "\n",
    "    # concatenate the params_list\n",
    "    params_list = [(workstep_query_name, bounds_list[i], chunk_size, RUN_ID) for i in range(len(bounds_list))]\n",
    "\n",
    "    # get end_date\n",
    "    end_date = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    # collect the log info\n",
    "    message = {\"idx\":workstep_idx}\n",
    "    message = json.dumps(message)\n",
    "    log_level = \"INFO\"\n",
    "\n",
    "    # write to log\n",
    "    with psycopg2.connect(connect_db) as conn:\n",
    "        queries.dml.write_to_log(conn,log_level=log_level,run_id=RUN_ID,start_date=start_date,end_date=end_date,work_step=spatial_bound_query_name,chunk_size=chunk_size,item_count=None,message=message)\n",
    "        conn.commit()\n",
    "\n",
    "    return params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a parallel processing workstep bounded by ID range\n",
    "\n",
    "def call_workstep(workstep_query_name: str, lower_bound: int, upper_bound: int, chunk_size: int, RUN_ID: int):\n",
    "    \"\"\"\n",
    "    Call a procedure for a workstep that can be\n",
    "    executed in parallel, like \"vertex_2_edge\".\n",
    "    \"\"\"\n",
    "    params = (lower_bound, upper_bound)\n",
    "    workstep_query = getattr(queries.dml, workstep_query_name).sql\n",
    "    with psycopg2.connect(connect_db) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            start_date = datetime.now(timezone.utc).isoformat()\n",
    "            cur.execute(workstep_query, params)\n",
    "            # get end_date\n",
    "            end_date = datetime.now(timezone.utc).isoformat()\n",
    "            item_count = (cur.fetchone())[0]\n",
    "            # collect the log info\n",
    "            message = {\"idx\":workstep_idx,\n",
    "                    \"concurrency\": CONCURRENCY,\n",
    "                    \"chunk_size\": chunk_size\n",
    "                    }\n",
    "            message = json.dumps(message)\n",
    "            log_level = \"INFO\"\n",
    "        # write to log\n",
    "        queries.dml.write_to_log(conn,log_level=log_level,run_id=RUN_ID,start_date=start_date,end_date=end_date,work_step=workstep_query_name,chunk_size=chunk_size,item_count=item_count,message=message)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def call_parallel_workstep(params_list, CONCURRENCY: int, chunk_size: int, workstep_query_name: str, workstep_idx: int, RUN_ID: int):\n",
    "    \"\"\"\n",
    "    Parallel call of a procedure.\n",
    "    \"\"\"\n",
    "    # get start_date\n",
    "    start_date = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    with mp.Pool(processes=CONCURRENCY) as pool:\n",
    "        pool.starmap(call_workstep, params_list)\n",
    "    \n",
    "    # get end_date\n",
    "    end_date = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    # collect the log info\n",
    "    message = {\"idx\":workstep_idx,\n",
    "            \"concurrency\": CONCURRENCY,\n",
    "            \"chunk_size\": chunk_size\n",
    "            }\n",
    "    message = json.dumps(message)\n",
    "    log_level = \"INFO\"\n",
    "\n",
    "    # write to log\n",
    "    with psycopg2.connect(connect_db) as conn:\n",
    "        queries.dml.write_to_log(conn,log_level=log_level,run_id=RUN_ID,start_date=start_date,end_date=end_date,work_step=workstep_query_name,chunk_size=chunk_size,item_count=None,message=message)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a parallel processing workstep bounded by geometry\n",
    "\n",
    "def call_spatial_workstep(spatial_workstep_query_name: str, selector_geometry: str, chunk_size: int, RUN_ID: int):\n",
    "    \"\"\"\n",
    "    Call a procedure for a workstep that can be\n",
    "    executed in parallel, like \"vertex_2_edge\".\n",
    "    \"\"\"\n",
    "    params = (selector_geometry,)\n",
    "    spatial_workstep_query = getattr(queries.dml, spatial_workstep_query_name).sql\n",
    "    with psycopg2.connect(connect_db) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            start_date = datetime.now(timezone.utc).isoformat()\n",
    "            cur.execute(spatial_workstep_query, params)\n",
    "            # get end_date\n",
    "            end_date = datetime.now(timezone.utc).isoformat()\n",
    "            item_count = (cur.fetchone())[0]\n",
    "            # collect the log info\n",
    "            message = {\"idx\":workstep_idx,\n",
    "                    \"concurrency\": CONCURRENCY,\n",
    "                    \"chunk_size\": chunk_size\n",
    "                    }\n",
    "            message = json.dumps(message)\n",
    "            log_level = \"INFO\"\n",
    "        # write to log\n",
    "        queries.dml.write_to_log(conn,log_level=log_level,run_id=RUN_ID,start_date=start_date,end_date=end_date,work_step=spatial_workstep_query_name,chunk_size=chunk_size,item_count=item_count,message=message)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def call_parallel_spatial_workstep(params_list, CONCURRENCY: int, chunk_size: int, workstep_query_name: str, workstep_idx: int, RUN_ID: int):\n",
    "    \"\"\"\n",
    "    Parallel call of a procedure.\n",
    "    \"\"\"\n",
    "    # get start_date\n",
    "    start_date = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    with mp.Pool(processes=CONCURRENCY) as pool:\n",
    "        pool.starmap(call_spatial_workstep, params_list)\n",
    "    \n",
    "    # get end_date\n",
    "    end_date = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    # collect the log info\n",
    "    message = {\"idx\":workstep_idx,\n",
    "            \"concurrency\": CONCURRENCY,\n",
    "            \"chunk_size\": chunk_size\n",
    "            }\n",
    "    message = json.dumps(message)\n",
    "    log_level = \"INFO\"\n",
    "\n",
    "    # write to log\n",
    "    with psycopg2.connect(connect_db) as conn:\n",
    "        queries.dml.write_to_log(conn,log_level=log_level,run_id=RUN_ID,start_date=start_date,end_date=end_date,work_step=workstep_query_name,chunk_size=chunk_size,item_count=None,message=message)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index a table for further processing\n",
    "\n",
    "def create_index(index_statement_name: str, workstep_idx: int, RUN_ID: int):\n",
    "    \"\"\"\n",
    "    Call index creation statement by query name.\n",
    "    \"\"\"\n",
    "    index_statement = getattr(queries.ddl, index_statement_name).sql\n",
    "    start_date = datetime.now(timezone.utc).isoformat()\n",
    "    with psycopg2.connect(connect_db) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(index_statement)\n",
    "        conn.commit()   \n",
    "    end_date = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    # collect the log info\n",
    "    message = {\"idx\":workstep_idx}\n",
    "    message = json.dumps(message)\n",
    "    log_level = \"INFO\"\n",
    "\n",
    "    # write to log\n",
    "    with psycopg2.connect(connect_db) as conn:\n",
    "        queries.dml.write_to_log(conn,log_level=log_level,run_id=RUN_ID,start_date=start_date,end_date=end_date,work_step=index_statement_name,chunk_size=None,item_count=None,message=message)\n",
    "        conn.commit()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *perform processing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find bounds in the poi table\n",
    "\n",
    "chunk_bound_query_name = 'find_bounds_in_poi_table'\n",
    "workstep_query_name = 'join_vertex_2_edge'\n",
    "workstep_idx += 1\n",
    "params_list = create_workstep_params_list(chunk_bound_query_name, CHUNK_SIZE, workstep_query_name, workstep_idx, RUN_ID)\n",
    "#params_list = params_list[:1]\n",
    "#params_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join vertices to the nearest edges\n",
    "\n",
    "workstep_idx += 1\n",
    "call_parallel_workstep(params_list,CONCURRENCY, CHUNK_SIZE, workstep_query_name, workstep_idx, RUN_ID)\n",
    "workstep_idx += 1\n",
    "create_index('create_index_vertex_2_edge_edge_id_idx', workstep_idx, RUN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find bounds in the vertex junction table\n",
    "\n",
    "chunk_bound_query_name = 'find_bounds_in_vertex_2_edge'\n",
    "workstep_query_name = 'process_junctions_and_edges'\n",
    "workstep_idx += 1\n",
    "params_list = create_workstep_params_list(chunk_bound_query_name, EDGE_PROCESSING_CHUNK_SIZE, workstep_query_name, workstep_idx, RUN_ID)\n",
    "#params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the junctions and edges (segmentize the near_net edges)\n",
    "\n",
    "#workstep_idx += 1\n",
    "call_parallel_workstep(params_list,CONCURRENCY, EDGE_PROCESSING_CHUNK_SIZE, workstep_query_name, workstep_idx, RUN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "# create spatial selector grid over the remaining road network edges\n",
    "workstep_idx += 1\n",
    "calculate_selector_grid(FAR_NET_PROCESSING_CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "# prepare the spatial bound parameter list for far_net edge processing\n",
    "\n",
    "spatial_bound_query_name = 'select_selector_grid'\n",
    "spatial_workstep_query_name = 'segmentize_road_network'\n",
    "workstep_idx += 1\n",
    "params_list = create_spatial_workstep_params_list(spatial_bound_query_name, FAR_NET_PROCESSING_CHUNK_SIZE, spatial_workstep_query_name, workstep_idx, RUN_ID)\n",
    "# params_list = params_list[:12]\n",
    "print(len(params_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the remaining road network edges (process the far_net edges)\n",
    "\n",
    "workstep_idx += 1\n",
    "call_parallel_spatial_workstep(params_list,6, FAR_NET_PROCESSING_CHUNK_SIZE, spatial_workstep_query_name, workstep_idx, RUN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the segments table\n",
    "\n",
    "workstep_idx += 1\n",
    "create_index('create_index_segments_geom_idx', workstep_idx, RUN_ID)\n",
    "workstep_idx += 1\n",
    "create_index('create_index_segments_node_1_idx', workstep_idx, RUN_ID)\n",
    "workstep_idx += 1\n",
    "create_index('create_index_segments_node_2_idx', workstep_idx, RUN_ID)\n",
    "workstep_idx += 1\n",
    "create_index('create_index_segments_edge_id_idx', workstep_idx, RUN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the spatial bound parameter list for node degree calculation\n",
    "\n",
    "spatial_bound_query_name = 'select_selector_grid'\n",
    "spatial_workstep_query_name = 'count_node_degree'\n",
    "workstep_idx += 1\n",
    "params_list = create_spatial_workstep_params_list(spatial_bound_query_name, FAR_NET_PROCESSING_CHUNK_SIZE, spatial_workstep_query_name, workstep_idx, RUN_ID)\n",
    "# params_list = params_list[:12]\n",
    "# print(len(params_list))\n",
    "# print(params_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the node degree from the segments table\n",
    "\n",
    "workstep_idx += 1\n",
    "call_parallel_spatial_workstep(params_list,6, FAR_NET_PROCESSING_CHUNK_SIZE, spatial_workstep_query_name, workstep_idx, RUN_ID)\n",
    "workstep_idx += 1\n",
    "create_index('create_index_nodes_node_id_idx', workstep_idx, RUN_ID)\n",
    "workstep_idx += 1\n",
    "create_index('create_index_nodes_selector_grid_hash_id_idx', workstep_idx, RUN_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
